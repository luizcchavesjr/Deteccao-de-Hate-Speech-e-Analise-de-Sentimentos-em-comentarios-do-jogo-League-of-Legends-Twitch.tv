{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Detecção de Hate Speech e Análise de Sentimentos de Comentários Transmissões de Streamers do Jogo League of Legends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dependências\n",
    "\n",
    "Estes são os requisitos necessários para seguir este tutorial:\n",
    "\n",
    "- [Python 3.x](https://www.python.org/downloads/)\n",
    "- [Jupyter Notebook](http://jupyter.org/install)\n",
    "- [Pandas](https://pandas.pydata.org)\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "- [Polyglot](http://polyglot.readthedocs.io/en/latest/Installation.html)\n",
    "- [Scikit-Learn](http://scikit-learn.org/stable/install.html)\n",
    "- [Scipy](https://www.scipy.org/install.html)\n",
    "- [Wordcloud](https://github.com/amueller/word_cloud)\n",
    "- [Matplotlib](https://matplotlib.org/users/installing.html)\n",
    "- [Hatesonar](https://github.com/Hironsan/HateSonar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Leitura dos dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Caminhos para os arquivos\n",
    "\n",
    "#filename = \"Dataset/C9Sneaky_comments.csv\"\n",
    "#filename = \"Dataset/DoubleLift_comments.csv\"\n",
    "#filename = \"Dataset/DoubleLift_comments.csv\"\n",
    "#filename = \"Dataset/lolTyler1_comments.csv\"\n",
    "#filename = \"Dataset/Nightblue3_comments.csv\"\n",
    "#filename = \"Dataset/Nightblue3_comments.csv\"\n",
    "#filename = \"Dataset/TFBlade_comments.csv\"\n",
    "#filename = \"Dataset/Trick2g_comments.csv\"\n",
    "filename = \"Dataset/TSM_Bjergsen_comments.csv\"\n",
    "#filename = \"Dataset/Yassuo_comments.csv\"\n",
    "#filename = \"Dataset/TwitchDataset.csv\" \n",
    "# Função que faz a leitura do dataset.\n",
    "def read_data(path, sep):\n",
    "    data = pd.read_csv(path, sep=sep)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = read_data(filename, sep=\"\\t\")\n",
    "print(dataset.comment.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Número de Visualizações por Canal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def ch_num_of_views(text):\n",
    "    now = []\n",
    "    strm = []\n",
    "    for data in text:\n",
    "        df_prof=pd.read_csv(data, sep='\\t')\n",
    "        df1=list(df_prof.iloc[0:1].user_name)[0]\n",
    "        df2=list(df_prof.iloc[0:1].user_vcount)[0]\n",
    "        df2=str(df2)\n",
    "        now.append(df2)\n",
    "        strm.append(df1)\n",
    "    return strm,now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caminhos para os arquivos\n",
    "\n",
    "#file_pro= = \"Profiles/C9Sneaky.csv\"\n",
    "#file_pro = \"Profiles/DoubleLift.csv\"\n",
    "#file_pro = \"Profiles/foggedftw2.csv\"\n",
    "#file_pro = \"Profiles/lolTyler1.csv\"\n",
    "#file_pro = \"Profiles/Nightblue3.csv\"\n",
    "#file_pro = \"Profiles/Rush.csv\"\n",
    "#file_pro = \"Profiles/TFBlade.csv\"\n",
    "#file_pro = \"Profiles/Trick2g.csv\"\n",
    "#file_pro = \"Profiles/Yassuo.csv\"\n",
    "#file_pro = \"Profiles/TSM_Bjergsen.csv\"\n",
    "file_pro = [\"Profiles/TSM_Bjergsen.csv\",\"Profiles/Yassuo.csv\",\"Profiles/Trick2g.csv\",\"Profiles/TFBlade.csv\",\"Profiles/Rush.csv\",\n",
    "            \"Profiles/Nightblue3.csv\",\"Profiles/lolTyler1.csv\",\"Profiles/foggedftw2.csv\",\n",
    "            \"Profiles/DoubleLift.csv\",\"Profiles/C9Sneaky.csv\"]\n",
    "           \n",
    "strm_list,now_list=ch_num_of_views(file_pro)\n",
    "dict_now_strm= {'Streamer':strm_list,'Número de Visualizações':now_list}\n",
    "df_now = pd.DataFrame(dict_now_strm,columns=['Streamer','Número de Visualizações'])\n",
    "df_now['Número de Visualizações']= df_now['Número de Visualizações'].astype('int')\n",
    "df_now=df_now.sort_values(by='Número de Visualizações',ascending=False)\n",
    "df_now.to_csv(r'Num_visualizações/Num_views_ch.csv', index = None, header=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file = \"Profiles/TSM_Bjergsen.csv\"\n",
    "df = pd.read_csv(file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_sep=w= 'Num_visualizações/Num_views_ch.csv'\n",
    "df_view=pd.read_csv(file_view,sep='\\t')\n",
    "plt.rcParams.update({'font.size':15})\n",
    "# Plotando gráfico de classificação por streamer\n",
    "ax=df_view[['Número de Visualizações']].plot(figsize=(10,5),kind='bar')\n",
    "ax.set_xticklabels(df_view['Streamer'], rotation=45)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Streamers')\n",
    "plt.ylabel('Número de Visualizações')\n",
    "plt.title('Número de Visualizações por Canal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 1 - Letras Minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Função que converte o texto em letras minúsculas.\n",
    "def lower_case(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2 - Removendo Acentuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Para realizar essa tarefa é necessário importar a biblioteca \"unicodedata\".\n",
    "import unicodedata\n",
    "# Função que remove a acentuação do texto, onde 'NFKD' é o codigo da forma normal para entrada de strings unicode.\n",
    "def remove_accentuation(text):\n",
    "    text = unicodedata.normalize('NFKD', str(text)).encode('ASCII','ignore')\n",
    "    return text.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3 - Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Função que faz a tokenização do texto.\n",
    "def tokenize(text):\n",
    "    return text.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4 - Remoção de Ruídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Função que remove pontuação e substitui alguns termos e caracteres do texto.\n",
    "def remove_noise(text):\n",
    "    # re.sub(replace_expression, replace_string, target)\n",
    "    new_text = re.sub(r\"\\.|,|;|!|\\?|\\\"|\\:|\\+|\\-|\\*|\\#|\\'|\\~\",'', text)\n",
    "    new_text = re.sub(r\"@\\S+\", \"MENTION\",new_text)\n",
    "    new_text = re.sub(r\"\\<3+\", \"LOVE\", new_text)\n",
    "    new_text = re.sub(r\"\\bf\\b\", \"FLASH\", new_text)\n",
    "    new_text = re.sub(r\"[A-Z0-9]+|\\.|\\!|\\,\",'', new_text)\n",
    "    new_text = re.sub(r\"\\$|\\@|\\(|\\)|\\&|\\¨|\\_|\\=\",'', new_text)\n",
    "    new_text = re.sub(r\"\\\\|\\||\\/|\\>|\\<|\\[|\\]|\\{|\\}\",'', new_text)\n",
    "    new_text = re.sub(r\"\\n|\\t|\\r\",'', new_text)\n",
    "    new_text = re.sub(r\"\\`|\\´|\\^|\\%|\\;|\\:|\\§|\\ª|\\º|\\₢|\\°\",'', new_text)\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Remoção de Números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Função para remover números do texto\n",
    "def remove_numbers(text):\n",
    "    # re.sub(replace_expression, replace_string, target)\n",
    "    new_text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Remoção de Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stop = set(stopwords.words('english'))# indica o conjunto de stopwords específico do idioma inglês.\n",
    "# Função para remover stop words do texto.\n",
    "def remove_stop_words(text, stopWords):\n",
    "    for sw in stopWords:\n",
    "        text = re.sub(r'\\b%s\\b' % sw, \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 7 - Redução de Comprimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Padrão de comprimento de palavras utilizado\n",
    "lengthening_pattern = \"a{3,}|b{3,}|c{3,}|d{3,}|e{3,}|f{3,}|g{3,}|h{3,}|i{3,}|j{3,}|\" \\\n",
    "                        \"k{3,}|l{3,}|m{3,}|n{3,}|o{3,}|p{3,}|q{3,}|r{3,}|s{3,}|t{3,}|\" \\\n",
    "                        \"u{3,}|v{3,}|x{3,}|w{3,}|y{3,}|z{3,}\"\n",
    "# Função usada para reduzir o comprimento de palavras no texto. Exemplo: 'looove' ----> 'love'.\n",
    "def lengthening_reduction(text, lenPattern):\n",
    "    lengthenings = re.findall(lenPattern, text)\n",
    "    if lengthenings:\n",
    "        lengthenings = lengthenings[0]\n",
    "        text = re.sub(lengthenings, lengthenings[0:1], text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 8 - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "# Instanciando o Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Funcão para Stemming\n",
    "def stemming(token, stemmer):\n",
    "    return stemmer.stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Instanciando o Snowball stemmer\n",
    "stemmer_en = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 9 - Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# The argument \"special_terms\" refers to terms that should not be parsed.\n",
    "def lemmatizing(text, special_terms=[]):\n",
    "    tokenPattern = r\"[A-Z0-9-']+|\\.|\\!|,\"\n",
    "    tokens = tokenize_refined(text, tokenPattern)\n",
    "    \n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for token, tag in tags:\n",
    "        if token in special_terms:\n",
    "            lemmatized_tokens.append(token)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag))\n",
    "            lemmatized_tokens.append(lemma)\n",
    "\n",
    "    lemmatized_tokens_string = \" \".join([token for token in lemmatized_tokens])\n",
    "    return lemmatized_tokens_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 - Removendo Online Social Networking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def removalOSN(text):\n",
    "    text = re.sub(\"#\\\\S+\", \"\", text)\n",
    "    text = re.sub(\"@\\\\S+\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para substituir expressões próprias do Twitch\n",
    "#def subTwitchslangs(text):\n",
    "#text = re.sub(\"\\bpog\\b\", \"surprised\", text)\n",
    "    #text = re.sub(\"\\bpogu\\b\", \"surprised\", text)\n",
    "    #text = re.sub(\"\\bpogger\\b\", \"surprised\", text)\n",
    "    #text = re.sub(\"\\bpogchamp\\b\", \"surprised\", text)\n",
    "    #text = re.sub(\"\\btrihard\\b\", \"excited\", text)\n",
    "    #text = re.sub(\"\\bkappa\\b\", \"sarcasm\", text)\n",
    "    #text = re.sub(\"\\bminik\\b\", \"sarcasm\", text)\n",
    "    #text = re.sub(\"\\bkappahd\\b\", \"sarcasm\", text)\n",
    "    #text = re.sub(\"\\bkappaross\\b\", \"sarcasm\", text)\n",
    "    #text = re.sub(\"\\bkappaclaus\\b\", \"sarcasm\", text)\n",
    "    #text = re.sub(\"\\bkappapride\\b\", \"sarcasm\", text)\n",
    "    #text = re.sub(\"\\bkeppo\\b\", \"sarcasm\", text)\n",
    "    #text = re.sub(\"\\bkappa\\b\", \"sarcasm\", text)\n",
    "    #text = re.sub(\"\\blul\\b\", \"sarcasm\", text)\n",
    "    #text = re.sub(\"\\bkekw\\b\", \"laugh\", text)\n",
    "    #text = re.sub(\"\\blmao\\b\", \"laugh\", text)\n",
    "    #text = re.sub(\"\\bpepehands\\b\", \"sad\", text)\n",
    "    #return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para substituir OSN\n",
    "def replacementOSN(text):\n",
    "    text = re.sub(\"#\\\\S+\", \"HASHTAG\", text)\n",
    "    \n",
    "    text = re.sub(\"@\\\\S+\", \"MENTION\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#função para interpretação de hashtags\n",
    "def interpretationHashtags(text):\n",
    "    hashtags = re.findall(\"#\\\\S+\", text)\n",
    "    for hashtag in hashtags:\n",
    "        words = re.findall('[a-zA-Z][^A-Z]*', hashtag)\n",
    "        wordsAsText = \" \".join([word for word in words])\n",
    "        text = re.sub(hashtag, wordsAsText, text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#função para remover urls\n",
    "def removeURL(text):\n",
    "    text = re.sub(\"http\\\\S+\\\\s*\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Transformação dos dados\n",
    "\n",
    "Nesta etapa serão chamadas as funções criados anteriormente para tratar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Caminhos para os arquivos\n",
    "\n",
    "#filename = \"Dataset/C9Sneaky_comments.csv\"\n",
    "#filename = \"Dataset/DoubleLift_comments.csv\"\n",
    "#filename = \"Dataset/DoubleLift_comments.csv\"\n",
    "#filename = \"Dataset/lolTyler1_comments.csv\"\n",
    "#filename = \"Dataset/Nightblue3_comments.csv\"\n",
    "#filename = \"Dataset/Nightblue3_comments.csv\"\n",
    "#filename = \"Dataset/TFBlade_comments.csv\"\n",
    "#filename = \"Dataset/Trick2g_comments.csv\"\n",
    "filename = \"Dataset/TSM_Bjergsen_comments.csv\"\n",
    "#filename = \"Dataset/Yassuo_comments.csv\"\n",
    "#filename = \"Dataset/TwitchDataset.csv\" \n",
    "dataset = read_data(filename, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(dataset.comment.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantidade de comentários brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Função que faz todo o preprocessamento, aplicando as funções criadas acima.\n",
    "def preprocessing(dataframe, fieldName, config):   \n",
    "    \n",
    "    if config[\"remove_numbers\"] == True:\n",
    "        dataframe[fieldName] = dataframe[fieldName].apply(remove_numbers)\n",
    "        \n",
    "    if config[\"lowercase\"] == True:\n",
    "        dataframe[fieldName] = dataframe[fieldName].apply(lower_case)\n",
    "    \n",
    "    if config[\"replace_hashtagsMentions\"] == True:\n",
    "        dataframe[fieldName] = dataframe[fieldName].apply(replacementOSN)\n",
    "        \n",
    "    if config[\"remove_accentuation\"] == True:\n",
    "        dataframe[fieldName] = dataframe[fieldName].apply(remove_accentuation)\n",
    "    \n",
    "    if config[\"remove_stopwords\"] == True:\n",
    "        dataframe[fieldName] = dataframe[fieldName].apply(remove_stop_words, stopWords=nltk_stop)\n",
    "    \n",
    "    if config[\"reduce_lengthening\"] == True:\n",
    "        dataframe[fieldName] = dataframe[fieldName].apply(lengthening_reduction, lenPattern=lengthening_pattern)\n",
    "    \n",
    "    if config[\"stemming\"] == True:\n",
    "        dataframe[fieldName] = dataframe[fieldName].apply(stemming, stemmer=stemmer)\n",
    "    \n",
    "    if config[\"lemmatizing\"] == True:\n",
    "        dataframe[fieldName] = dataframe[fieldName].apply(lemmatizing)\n",
    "    \n",
    "    if config[\"remove_hashtagsMentions\"] == True:\n",
    "        dataframe[fieldName] = dataframe[fieldName].apply(removalOSN)\n",
    "          \n",
    "    if config[\"interpretHashtags\"] == True:\n",
    "        dataframe[fieldName] = dataframe[fieldName].apply(interpretationHashtags)\n",
    "    \n",
    "    if config[\"remove_urls\"] == True:\n",
    "        dataframe[fieldName] = dataframe[fieldName].apply(removeURL)\n",
    "        \n",
    "    if config[\"remove_noise\"] == True:\n",
    "        dataframe[fieldName] = dataframe[fieldName].apply(remove_noise)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "            \"remove_numbers\": True, \\\n",
    "            \"lowercase\": True, \\\n",
    "            \"remove_accentuation\": True, \\\n",
    "            \"remove_stopwords\": True, \\\n",
    "            \"reduce_lengthening\": True, \\\n",
    "            \"stemming\": False, \\\n",
    "            \"lemmatizing\": False, \\\n",
    "            \"remove_hashtagsMentions\": True, \\\n",
    "            \"replace_hashtagsMentions\": True, \\\n",
    "            \"interpretHashtags\": True, \\\n",
    "            \"remove_urls\": True, \\\n",
    "            \"remove_noise\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset.dropna(inplace = True)# remove linhas com valores NAN\n",
    "dataset.drop_duplicates(inplace = True)# remove elementos duplicados\n",
    "dataset_pproc = preprocessing(dataset, fieldName=\"comment\", config=config)\n",
    "dataset_pproc.comment.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantidade de comentários após preprocessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_pproc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando os comentários preprocessados em um arquivo .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(dataset_pproc,columns=['comment'])\n",
    "#df.to_csv (r'PreprocessDataset/TwitchDataset_pproc_Dataset.csv', index = None, header=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Visualização em WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from os import path\n",
    "from wordcloud import WordCloud,STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Wordcloud para comentários\n",
    "text = ' '.join(dataset_pproc.comment.values)# retira uma lista com os valores contidos no dataset\n",
    "words = text.split()\n",
    "text = (\" \".join(sorted(set(words),key=words.index)))#organiza e seleciona apenas palavras unicas da lista\n",
    "text = re.sub(r'[^A-Za-z0-9 ]+','', text)# substitui possíveis ruídos no texto\n",
    "\n",
    "#print(text)\n",
    "# Gerando a imagem wordcloud\n",
    "wordcloud = WordCloud().generate(text)\n",
    "wordcloud = WordCloud(max_font_size=55,background_color=\"white\", max_words=50).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem de Tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Função para vetorização do dataset\n",
    "def transformData(data, fieldName, my_tokenizer, weight):\n",
    "    \n",
    "    if weight == \"TP\":\n",
    "        vectorizer = CountVectorizer(tokenizer=my_tokenizer, binary=True)\n",
    "        X = vectorizer.fit_transform(data[fieldName])\n",
    "    \n",
    "    elif weight == \"TF\":\n",
    "        vectorizer = CountVectorizer(tokenizer=my_tokenizer)\n",
    "        X = vectorizer.fit_transform(data[fieldName])\n",
    "        \n",
    "    elif weight == \"TFIDF\":\n",
    "        vectorizer = TfidfVectorizer(tokenizer=my_tokenizer)\n",
    "        X = vectorizer.fit_transform(data[fieldName])\n",
    "\n",
    "    return (vectorizer, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Chamando função para vetorizar o dataset\n",
    "vectorizer, X = transformData(dataset_pproc, fieldName=\"comment\", my_tokenizer=tokenize, weight=\"TFIDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#variável contento as caracteristicas extraídas do dataset\n",
    "tfidf_feature_names= vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "\n",
    "#Função para mostrar os principais tópicos\n",
    "def display_topics(model, feature_names, top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Tópico %d: \" % (topic_idx) +\n",
    "                \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-top_words -2:-1]]))\n",
    "\n",
    "topics = 5\n",
    "top_words = 5\n",
    "# Exexutando os modelos LDA e LSA \n",
    "lda = LatentDirichletAllocation(n_components=topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(X)\n",
    "lsa = TruncatedSVD(n_components=topics, algorithm='randomized', n_iter=5, random_state=1).fit(X)\n",
    "\n",
    "#Mostrando os resultados para cada modelo\n",
    "print('Modelo LDA')\n",
    "display_topics(lda, tfidf_feature_names, top_words)\n",
    "print('Modelo LSA')\n",
    "display_topics(lsa, tfidf_feature_names, top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#print(\"{:<16}{}\".format(\"Word\", \"Polarity\")+\"\\n\"+\"-\"*30)\n",
    "\n",
    "def sent_analize_pos(text):\n",
    "    Positivos = []\n",
    "    for w in text.words:\n",
    "        #print(w.polarity)\n",
    "        if (w.polarity == 1):\n",
    "            Positivos.append(\"{:<16}{:>2}\".format(w, w.polarity))\n",
    "    return Positivos\n",
    "\n",
    "def sent_analize_neg(text):\n",
    "    Negativos = []\n",
    "    for w in text.words:\n",
    "        #print(w.polarity)\n",
    "        if (w.polarity == -1):\n",
    "            Negativos.append(\"{:<16}{:>2}\".format(w, w.polarity))\n",
    "    return Negativos\n",
    "\n",
    "def sent_analize_neutros(text):\n",
    "    Neutros = []\n",
    "    for w in text.words:\n",
    "        #print(w.polarity)\n",
    "        if (w.polarity == 0):\n",
    "            Neutros.append(\"{:<16}{:>2}\".format(w, w.polarity))\n",
    "    return Neutros\n",
    "text = Text(' '.join(dataset_pproc.comment.values))\n",
    "Positivos = sent_analize_pos(text)\n",
    "Negativos = sent_analize_neg(text)\n",
    "Neutros = sent_analize_neutros(text)\n",
    "Num_pos= len(Positivos)\n",
    "Num_neg= len(Negativos) \n",
    "Num_neutros= len(Neutros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "labels = 'Positivos', 'Negativos', 'Neutros', \n",
    "sizes = [Num_pos, Num_neg, Num_neutros]\n",
    "explode = (0.1, 0.1, 0.1)  # Explode all sides\n",
    "plt.rcParams.update({'font.size':15})\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=200)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.title('Análise de Sentimentos dos comentários')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\" Positivos: %d\\n\" % Num_pos,\n",
    "      \"Negativos: %d \\n\" % Num_neg,\n",
    "      \"Neutros: %d\" % Num_neutros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecção de HateSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import pandas as pd\n",
    "from hatesonar import Sonar\n",
    "\n",
    "# Função que utiliza o hatesonar para classificar os comentãrios linha por linha em hate speech, offensive language e neither\n",
    "# A função salva os resultados em um arquivo .csv\n",
    "def detect_hs(text):\n",
    "    warnings.filterwarnings(action='ignore')\n",
    "    sonar = Sonar()\n",
    "    with open('HS_detection/TwitchDataset_HS_detect_saved.csv','w',encoding='utf8') as f:\n",
    "        titles = f\"comment\\tTop_Class\\tHateSpeech_Confidence\\tOfensive_Language_Confidence\\tNeither_Confidence\\n\"\n",
    "        f.write(titles)\n",
    "        temp_list = []\n",
    "        for dado in text.itertuples():\n",
    "            dado = dado.comment\n",
    "            dit = sonar.ping(dado)# método que faz a classificação: o resultado é um dicionário contendo os resultad\n",
    "            texto = dit['text']\n",
    "            classif = dit['top_class']\n",
    "            classes = dit['classes']\n",
    "            hs_confidence = classes[0]['confidence']\n",
    "            #hs_class = classes[0]['class_name']\n",
    "            ol_confidence = classes[1]['confidence']\n",
    "            #ol_class = classes[1]['class_name']\n",
    "            neither_confidence = classes[2]['confidence']\n",
    "            #neither_class = classes[2]['class_name']\n",
    "            temp_list.append(texto + classif + str(hs_confidence)  + str(ol_confidence) + str(neither_confidence))\n",
    "\n",
    "            f.write(f\"{texto}\\t{classif}\\t{hs_confidence}\\t{ol_confidence}\\t{neither_confidence}\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executando a função de detecção de hate speech\n",
    "file = 'PreprocessDataset/TwitchDataset_pproc_Dataset.csv'# caminho para o arquivo preprocessado desejado\n",
    "df = pd.read_csv(file,sep='\\t')\n",
    "df=df.dropna(subset=['comment'])\n",
    "df =detect_hs(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização dos Resultados da Detecção de Hate Speech por Streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lista com os caminhos para os arquivos de detecção de hate speech de cada streamer\n",
    "file = ['HS_detection/C9sneaky_HS_detect_saved.csv','HS_detection/DoubleLift_HS_detect_saved.csv','HS_detection/foggedftw2_HS_detect_saved.csv',\n",
    "        'HS_detection/lolTyler1_HS_detect_saved.csv','HS_detection/Nightblue3_HS_detect_saved.csv','HS_detection/Rush_HS_detect_saved.csv',\n",
    "        'HS_detection/TFBlade_HS_detect_saved.csv','HS_detection/Trick2g_HS_detect_saved.csv','HS_detection/TSM_Bjergsen_HS_detect_saved.csv',\n",
    "        'HS_detection/Yassuo_HS_detect_saved.csv','HS_detection/TwitchDataset_HS_detect_saved.csv']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Função para contagem das classificações por streamer e de todo o dataset\n",
    "\n",
    "def hs_for_stmrs(text):\n",
    "    hate_speech = []\n",
    "    of_lang = []\n",
    "    neither= []\n",
    "    total = 0\n",
    "    for dado in text.itertuples():\n",
    "        dados = dado.Top_Class\n",
    "        commt= dado.comment\n",
    "        if (dados == 'hate_speech'):\n",
    "            hate_speech.append(dados+':'+commt)\n",
    "            num_hs = len(hate_speech)\n",
    "           \n",
    "        elif(dados == 'offensive_language'):\n",
    "            of_lang.append(dados+':'+commt)\n",
    "            num_of_lg = len(of_lang)\n",
    "\n",
    "        elif(dados == 'neither'):\n",
    "            neither.append(dados+':'+commt)\n",
    "            num_nt = len(neither)\n",
    "           \n",
    "    total = (num_hs+num_of_lg+num_nt)\n",
    "    return num_hs,num_of_lg,num_nt,total\n",
    "\n",
    "# Iteração sobre a lista de caminhos de arquivos\n",
    "for i in file:\n",
    "    if (i== 'HS_detection/C9sneaky_HS_detect_saved.csv'):\n",
    "        strm1  = 'C9sneaky'\n",
    "        df2= pd.read_csv(i,sep='\\t')\n",
    "        hs1,ol1,nt1,tot1 = hs_for_stmrs(df2)\n",
    "    \n",
    "    elif (i== 'HS_detection/DoubleLift_HS_detect_saved.csv'):\n",
    "        strm2  = 'DoubleLift'\n",
    "        df2 = pd.read_csv(i,sep='\\t')\n",
    "        hs2,ol2,nt2,tot2 = hs_for_stmrs(df2)\n",
    "\n",
    "    elif (i== 'HS_detection/foggedftw2_HS_detect_saved.csv'):\n",
    "        strm3  = 'foggedftw2'\n",
    "        df2 = pd.read_csv(i,sep='\\t')\n",
    "        hs3,ol3,nt3,tot3 = hs_for_stmrs(df2)\n",
    "\n",
    "    elif (i== 'HS_detection/lolTyler1_HS_detect_saved.csv'):\n",
    "        strm4  = 'lolTyler1'\n",
    "        df2 = pd.read_csv(i,sep='\\t')\n",
    "        hs4,ol4,nt4,tot4 = hs_for_stmrs(df2)\n",
    "\n",
    "    elif (i== 'HS_detection/Nightblue3_HS_detect_saved.csv'):\n",
    "        strm5  = 'Nightblue3'\n",
    "        df2 = pd.read_csv(i,sep='\\t')\n",
    "        hs5,ol5,nt5,tot5 = hs_for_stmrs(df2)\n",
    "\n",
    "    elif (i== 'HS_detection/Rush_HS_detect_saved.csv'):\n",
    "        strm6  = 'Rush'\n",
    "        df2 = pd.read_csv(i,sep='\\t')\n",
    "        hs6,ol6,nt6,tot6 = hs_for_stmrs(df2)\n",
    "\n",
    "    elif (i== 'HS_detection/TFBlade_HS_detect_saved.csv'):\n",
    "        strm7   = 'TFBlade'\n",
    "        df2 = pd.read_csv(i,sep='\\t')\n",
    "        hs7,ol7,nt7,tot7 = hs_for_stmrs(df2)\n",
    "            \n",
    "    elif (i== 'HS_detection/Trick2g_HS_detect_saved.csv'):\n",
    "        strm8 = 'Trick2g'\n",
    "        df2 = pd.read_csv(i,sep='\\t')\n",
    "        hs8,ol8,nt8,tot8 = hs_for_stmrs(df2)\n",
    "\n",
    "    elif (i== 'HS_detection/TSM_Bjergsen_HS_detect_saved.csv'):\n",
    "        strm9  = 'TSM_Bjergsen'\n",
    "        df2 = pd.read_csv(i,sep='\\t')\n",
    "        hs9,ol9,nt9,tot9 = hs_for_stmrs(df2)\n",
    "\n",
    "    elif (i== 'HS_detection/Yassuo_HS_detect_saved.csv'):\n",
    "        strm10 = 'Yassuo'\n",
    "        df2 = pd.read_csv(i,sep='\\t')\n",
    "        hs10,ol10,nt10,tot10 = hs_for_stmrs(df2)\n",
    "    \n",
    "    elif (i== 'HS_detection/TwitchDataset_HS_detect_saved.csv'):# caminho para todo o dataset com todos os comentários\n",
    "        strm11 = 'TwitchDataset_All Data'\n",
    "        df2 = pd.read_csv(i,sep='\\t')\n",
    "        hs11,ol11,nt11,tot11 = hs_for_stmrs(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unindo os resultados em um lista para transformá-los em um dataframe pandas\n",
    "qt_hs=[hs1,hs2,hs3,hs4,hs5,hs6,hs7,hs8,hs9,hs10]\n",
    "strms= [strm1,strm2,strm3,strm4,strm5,strm6,strm7,strm8,strm9,strm10]\n",
    "qt_ol =[ol1,ol2,ol3,ol4,ol5,ol6,ol7,ol8,ol9,ol10] \n",
    "qt_nt =[nt1,nt2,nt3,nt4,nt5,nt6,nt7,nt8,nt9,nt10]\n",
    "tot =[tot1,tot2,tot3,tot4,tot5,tot6,tot7,tot8,tot9,tot10]\n",
    "dict_Hs = {'Streamer':strms,'Hate Speech':qt_hs,'Offensive Language':qt_ol, 'Neither':qt_nt,'Total':tot}\n",
    "dict_Alldata = {'All Twitch Dataset':[strm11],'Hate Speech':[hs11],'Offensive Language':[ol11], 'Neither':[nt11],'Total':[tot11]}\n",
    "# Transformando os resultados em um dataframe pandas\n",
    "df12 = pd.DataFrame(dict_Hs,columns=['Streamer','Hate Speech','Offensive Language','Neither','Total'])\n",
    "df13 = pd.DataFrame(dict_Alldata,columns=['All Twitch Dataset','Hate Speech','Offensive Language','Neither','Total'])\n",
    "# Salvando resultados em um arquivo .csv\n",
    "#df12.to_csv (r'hs_strmrs.csv', index = None, header=True,sep='\\t',)\n",
    "#df13.to_csv (r'hs_alldataset.csv', index = None, header=True,sep='\\t',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando em um Gráfico de Barras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'font.size':15})\n",
    "# Plotando gráfico de classificação por streamer\n",
    "ax=df12[['Hate Speech','Offensive Language','Neither']].plot(figsize=(15,5),kind='bar')\n",
    "ax.set_xticklabels(df12['Streamer'], rotation=45)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Streamers')\n",
    "plt.ylabel('Quantidade por Classe')\n",
    "plt.title('Classificação de Hate Speech, Offensive Language e Neither por Streamer')\n",
    "\n",
    "#Plotando gráfico de classificação de todo o dataset\n",
    "ax2=df13[['Hate Speech','Offensive Language','Neither']].plot(figsize=(10,5),kind='bar')\n",
    "ax2.set_xticklabels(df13['All Twitch Dataset'], rotation=0)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Comentários de todo o Dataset')\n",
    "plt.ylabel('Quantidade por Classe')\n",
    "plt.title('Classificação de Hate Speech,Offensive Language e Neither para todo o Dataset')\n",
    "\n",
    "#print(\" Hate Speech: %d\\n\" % hs11,\n",
    "      #\"Offenseive Language: %d \\n\" % ol11,\n",
    "      #\"Neither: %d\" % nt11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
